# Spark Kafka Comsumer Code Analysis

# Spark基本运行原理

们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。

在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。

Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。

当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。

因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。Executor内存分配这一块在Spark 2.x好像有变化。

task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。

# Spark Streaming Consumer代码

```
    // Create context with 2 second batch interval
    val sparkConf = new SparkConf().setAppName("DirectKafkaWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(2))

    // Create direct kafka stream with brokers and topics
    val topicsSet = topics.split(",").toSet
    val kafkaParams = Map[String, Object](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> brokers,
      ConsumerConfig.GROUP_ID_CONFIG -> groupId,
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer],
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer])
    val messages = KafkaUtils.createDirectStream[String, String](
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe[String, String](topicsSet, kafkaParams))
```

这里就从KafkaUtils下面的createDirectStream这个函数开始，这个函数返回类型是ConsumerRecord[K, V]

# KafkaUtils

我们先来看看KafkaUtils这个Object，注释是object for constructing Kafka streams and RDDs，提供了Kafka streams到RDDs转换的相关函数

首先我们调用的是

```
  def createDirectStream[K, V](
      ssc: StreamingContext,
      DStream: LocationStrategy,
      consumerStrategy: ConsumerStrategy[K, V],
      perPartitionConfig: PerPartitionConfig
    ): InputDStream[ConsumerRecord[K, V]] = {
    new DirectKafkaInputDStream[K, V](ssc, locationStrategy, consumerStrategy, perPartitionConfig)
  }
```

* StreamingContext : Spark Streaming的主入口，提供了从不同数据源创建DStream的方法及上下文所需要的一些方法

* LocationStrategy ： 这个四个可选，通常都是PreferConsistent，作用是Choice of how to schedule consumers for a given TopicPartition on an executor.

        PreferBrokers：Use this only if your executors are on the same nodes as your Kafka brokers.
        PreferConsistent： Use this in most cases, it will consistently distribute partitions across all executors.
        PreferFixed： Use this to place particular TopicPartitions on particular hosts if your load is uneven.
    
* ConsumerStrategy[K, V] ： 主要是怎么创建consumer，采用手动管理还是自动管理offset的方式，或者是正则匹配topic，这个主要在driver端执行构建一个consumer

        Subscribe ： Subscribe to a collection of topics.
        SubscribePattern ： Subscribe to all topics matching specified pattern to get dynamically assigned partitions
        Assign ： Assign a fixed collection of TopicPartitions
    
* PerPartitionConfig ： 通常都是调用下面的默认配置，就是获取每个分区的消费速率

        private class DefaultPerPartitionConfig(conf: SparkConf)
            extends PerPartitionConfig {
          val maxRate = conf.getLong("spark.streaming.kafka.maxRatePerPartition", 0)
          def maxRatePerPartition(topicPartition: TopicPartition): Long = maxRate
        }

KafkaUtils里面主要有三个函数createDirectStream、createRDD、fixKafkaParams，后面两个后面用到时再看

接着这里会实例化DirectKafkaInputDStream类，DirectKafkaInputDStream继承了InputDStream，InputDStream继承了DStream

# DStream & InputDStream

先来看看这两个框架层面的抽象类

***DStream***

直接看注释吧，清晰明了

A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a `continuous
sequence of RDDs (of the same type) representing a continuous stream of data` (see
org.apache.spark.rdd.RDD in the Spark core documentation for more details on RDDs).
DStreams can either be created from live data (such as, data from TCP sockets, Kafka,
etc.) using a [[org.apache.spark.streaming.StreamingContext]] or it can be generated by
transforming existing DStreams using operations such as `map`,
`window` and `reduceByKeyAndWindow`. While a Spark Streaming program is running, each DStream
periodically generates a RDD, either from live data or by transforming the RDD generated by a
parent DStream.

This class contains the basic operations available on all DStreams, such as `map`, `filter` and
`window`. In addition, [[org.apache.spark.streaming.dstream.PairDStreamFunctions]] contains
operations available only on DStreams of key-value pairs, such as `groupByKeyAndWindow` and
`join`. These operations are automatically available on any DStream of pairs
(e.g., DStream[(Int, Int)] through implicit conversions.

A DStream internally is characterized by a few basic properties:
 - A list of other DStreams that the DStream depends on
 - A time interval at which the DStream generates an RDD
 - A function that is used to generate an RDD after each time interval

```
  // =======================================================================
  // Methods that should be implemented by subclasses of DStream
  // =======================================================================

  /** Time interval after which the DStream generates an RDD */
  def slideDuration: Duration

  /** List of parent DStreams on which this DStream depends on */
  def dependencies: List[DStream[_]]

  /** Method that generates an RDD for the given time */
  def compute(validTime: Time): Option[RDD[T]]
```

这三个函数重点关注，特别是computer，在DirectKafkaInputDStream类中会重写，主要的作用是在给定的时间生成RDD

***InputDStream***

This is the abstract base class for all input streams. This class provides methods
`start() and stop() which are called by Spark Streaming system to start and stop
receiving data`, respectively.

Input streams that can generate RDDs from new data by running a service/thread only on
the driver node (that is, without running a receiver on worker nodes), can be
implemented by directly inheriting this InputDStream.

# DirectKafkaInputDStream

```
A DStream where each given Kafka topic/partition corresponds to an RDD partition. The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number of messages per second that each '''partition''' will accept.
```

KafkaUtils中的createDirectStream最终会将参数传入DirectKafkaInputDStream这个类中

直接参考：[DirectKafkaInputDStream源码分析(包含动态分区感知)](https://blog.csdn.net/qq_36421826/article/details/81660915)
